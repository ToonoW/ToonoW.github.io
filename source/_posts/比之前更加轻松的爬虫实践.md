---
title: 比之前更加轻松的爬虫实践
date: 2017-08-17 15:51:57
tags:
- 爬虫
- Scrapy
- Python
categories:
- Python
- 爬虫

---

在之前曾经做过好几个爬虫项目，因为当时刚刚使用爬虫的技术，一股蛮力地写爬取规则和解析存储规则，无论是不是同一种类型的爬虫，每个都给写一个单独使用的`pipeline`和`item`类。这种笨方法吃力不讨好，完全没有发挥Scrapy的特性，启动不同的爬虫还要去`setting.py`文件去注释掉和此次爬虫无关的`pipeline`。

时隔了几个月，又有写爬虫的需求。是爬取健康分类的资讯，需要爬取不同网站的同类型内容，存储到我的MongoDB数据库。这里选用MongoDB是因为它适合文档的存储，而且随时可以拓展字段，使用起来比传统的SQL数据库简便多了。

先看看我们的项目结构

```shell
NewsSpiders
├── __init__.py
├── items.py
├── middlewares.py
├── pipelines.py
├── settings.py
└── spiders
    ├── __init__.py
    └── sina.py
```

### 编码上的思路

对于入门者来说，最难的不是怎么把框架用起来，而是怎样才是使用框架的更优实践。我自己的话，通常是一开始费尽心机把程序写起来，只求能用。但是不断地追加代码之后，会越来越觉得项目已经被设计不合理拖垮了，一心只求更好的实践去重构。所以接下来就记录一些比粗暴方法更好的实践。

* **items.py -** 在这里定义我们资讯类通用的数据结构，无外乎标题、主体内容、来源媒体、来源网址、内容里的图片链接、图片的存储路径等；
* **spiders/ -** 文件夹下面放我们具体的爬虫，习惯性按照文件区分爬虫，我这里只有一个`sina.py`文件，也就是只有一个爬虫，实际上我们可以增加多个爬虫文件。另外，这里面的爬虫得讲究，需要最终产出的`item`格式尽可能相仿，这样子整个爬虫项目的数据清洗处理流程才能最大限度地复用，即不同爬虫产出的`item`每个字段的格式相同，例如标题都是纯文本，而主体都是`HTML`；
* **middlewares.py -** 这里存放中间件，在我的理解是爬虫的钩子。例如我在这里编写了修改`Headers`的中间件，重写了`UserAgentMiddleware`的 `process_request`方法，对请求的头进行了处理。几乎爬虫全部生命周期都可以在这里进行钩子处理；
* **pipelines.py -** 这里就是`item`被处理的管道了，其实也就是工厂里的流水线，每一个`pipelines`里不同功用的类就是一个负责不同操作的流水线工人，通过`setting.py`内的配置进行优先级的设置，进而控制执行的顺序。在这里面我将去重、抽取图片URL、下载图片、存储`item`都独立成一个`pipeline`类，按步骤进行处理；

下面是可参考的源码展示，spiders部分大多数是不可参考的，因为每个网站都是特异的。

```python
"""
items.py
"""
import scrapy
from scrapy import Field


class NewsspidersItem(scrapy.Item):
    title = Field()
    body = Field()#
    tags = Field()
    source = Field()            # 来源名称
    source_url = Field()        # 来源URL
    datetime = Field()          # 时间
    intro = Field()             # 介绍
    image_urls = Field()
    images = Field()            # 利用{index}进行占位，使用的时候替换上图片地址
    image_paths = Field()       # 图片路径列表
```

```python
"""
sina.py
"""
import json
import re
import logging
from scrapy.spider import CrawlSpider
from scrapy import Request

from ..items import CrnewsspidersItem


# 新浪健康 疾病
DISEASE_API_URL = '省略'
# 新浪健康 养生
HEALTH_CARE_API_URL = '省略'

class SinaSpider(CrawlSpider):
    name = 'sina'
    start_urls = [
        DISEASE_API_URL + '1',
        HEALTH_CARE_API_URL + '1',
    ]
    handle_httpstatus_list = [200, 403]

    def parse(self, response):
        result = {}
        try:
            result = json.loads(response.body)
            logging.error(type(response.body))
        except:
            AttributeError('输入json出错')
            logging.error('输入json出错')
            logging.error(response.body)
        data = result.get('result')
        if data is not None:
            data = data.get('data')
        else:
            ValueError('json中result.data内容应不为空')
            data = []
        if len(data) is not 0:
            # 爬取文章
            for page in data:
                info = {
                    'source_url': page['url'],
                    'source': page['media_name'],
                    'title': page['title'],
                    'datetime': page['ctime'],
                    'tags': page['keywords'].split(',') if page['keywords'] != '' else []
                }
                yield Request(page['url'], callback=self.parse_page, meta={'info': info})
            # 爬取下一页
            page_num = int(re.split(r'page=', response.url)[-1])
            prefix = ''
            if DISEASE_API_URL in response.url:
                prefix = DISEASE_API_URL
            else:
                prefix = HEALTH_CARE_API_URL
            url = prefix + str(page_num + 1)
            yield Request(url, callback=self.parse, headers={'Referer': ''})        # 接口Referer不能有值，否则会403


    def parse_page(self, response):
        info = response.meta['info']
        item = CrnewsspidersItem()
        for key in info:
            item[key] = info[key]
        # 等待处理body中的图片链接
        item['body'] = response.xpath('//div[@id="artibody"]')
        return item
```

```python
"""
middlewares.py
"""
import random
from scrapy import signals
from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware


class MyAgentMiddleware(UserAgentMiddleware):
    """
    随机设置请求头的user_agent
    """
    user_agent_list = [
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
        "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",
        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5",
        "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
        "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24",
        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"
    ]

    def __init__(self, user_agent=''):
        super().__init__()
        self.user_agent_len = len(self.user_agent_list)

    def process_request(self, request, spider):
        request.headers['User-Agent'] = self.user_agent_list[random.randrange(0, self.user_agent_len)]
```

```python
"""
pipelines.py
"""
import pymongo
import logging
from scrapy import Request
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem


class CrnewsspidersPipeline(object):

    def process_item(self, item, spider):
        return item


class CollectImageUrlPipeline(object):
    """
    将body中的image URL取出存入image_urls待下载
    """
    def process_item(self, item, spider):
        body = item['body']
        urls = body.xpath('.//img').xpath('@src').extract()
        for url in urls:
            if 'http' not in url:
                logging.warning('图片URL不完整： {}', url)
                urls.remove(url)
        item['image_urls'] = urls
        logging.info("图片数目: " + str(len(item['image_urls'])))
        return item


class ImagesDownloadPipeline(ImagesPipeline):
    # 下载图片
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            yield Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
        if not image_paths:
            item['image_paths'] = []
        item['image_paths'] = image_paths
        return item


class MongoDBPipeline(object):
    """
    存储到MongoDB
    """
    collection_name = 'news_items'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        item['body'] = item['body'].extract()
        self.db[self.collection_name].insert(dict(item))


class DuplicatesPipeline(MongoDBPipeline):
    """
    初步去重
    """
    def process_item(self, item, spider):
        result = self.db[self.collection_name].find({'source_url': item['source_url']})
        if result.count() != 0:
            raise DropItem("Duplicate item found: %s" % item)
        else:
            return item
```

```python
"""
setting.py
"""
BOT_NAME = 'NewsSpiders'

SPIDER_MODULES = ['NewsSpiders.spiders']
NEWSPIDER_MODULE = 'NewsSpiders.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Disable cookies (enabled by default)
COOKIES_ENABLED = False

# Enable or disable downloader middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
#    'CRNewsSpiders.middlewares.MyCustomDownloaderMiddleware': 543,
    'CRNewsSpiders.middlewares.MyAgentMiddleware': 544,
}

# Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    # 'CRNewsSpiders.pipelines.CrnewsspidersPipeline': 300,
    'CRNewsSpiders.pipelines.DuplicatesPipeline': 300,
    'CRNewsSpiders.pipelines.CollectImageUrlPipeline': 301,
    'CRNewsSpiders.pipelines.ImagesDownloadPipeline': 302,
    'CRNewsSpiders.pipelines.MongoDBPipeline': 310,
}

# MongoDB Database Info
MONGO_URI = 'mongodb://localhost'
MONGO_DATABASE = 'news'

# 图片存储
IMAGES_STORE = './images'
# 避免重复下载，设置图片过期时间
IMAGES_EXPIRES = 30
```

### 更加简单的部署

经过多番寻找和纠结，我最终选用了`scrapyd`进行爬虫的部署。

优点很多，但主要是达到了我的几个需求：

1. 使用网络API JSON接口进行控制爬虫项目群，方便之后拓展友好的用户界面；
2. 使用简单。简化了Scrapy的使用。
3. 部署简单。服务端程序运行之后，在本地项目添加对应的地址信息就可以同步到生产环境。(不过建议开发环境和生产环境使用统一版本Python,并且搭配Supervisor使用);